{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28745151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from url_parser import parse_url, get_url, get_base_url\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afbe2bd",
   "metadata": {},
   "source": [
    "### 1. Break large file to 9 smaller ones: \n",
    "Run this in terminal:\n",
    "\n",
    "```split -l 500000 -a 4 dataset.json smaller_```\n",
    "\n",
    "Rename the files to `1 - 9.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b596392",
   "metadata": {},
   "source": [
    "### 2. Convert json to csv file with selected column names\n",
    "1. first convert all the individual json files to csv files\n",
    "2. get all the column names in all the files\n",
    "3. select which colums to use, and save those files to a csv folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78da8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_csv():\n",
    "    for i in tqdm(range(1, 10)):\n",
    "        data = pd.read_json(f'../data/json/{i}.json', lines = True)\n",
    "        data.to_csv(f'../data/json/{i}.csv')\n",
    "\n",
    "def get_cols_name():\n",
    "    cols = set()\n",
    "    for i in tqdm(range(1, 10)):\n",
    "        data = pd.read_csv(f'../data/csv/{i}.csv', low_memory = False)\n",
    "        cols.update(data.columns.values.tolist())\n",
    "    return cols\n",
    "\n",
    "def use_cols_csv():\n",
    "    for i in tqdm(range(1, 10)):\n",
    "        data = pd.read_csv(\n",
    "            f'../data/json/{i}.csv', \n",
    "            low_memory = False, \n",
    "            usecols = ['browserFamily', 'channel', 'deviceType', 'iabCategories', 'os', 'refDomain', 'url']\n",
    "        )\n",
    "        data.to_csv(f'../data/csv/{i}.csv')\n",
    "\n",
    "def json_to_csv(print_cols = False):\n",
    "    convert_json_csv()\n",
    "    if print_cols:\n",
    "        cols = get_cols_name()\n",
    "        pprint(cols)\n",
    "    use_cols_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff051f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 9/9 [03:44<00:00, 24.97s/it]\n",
      "100%|█████████████████████████████████████████████| 9/9 [00:35<00:00,  3.94s/it]\n"
     ]
    }
   ],
   "source": [
    "json_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d43b5",
   "metadata": {},
   "source": [
    "### 3. Clean csv file\n",
    "1. seperate labels and no labels\n",
    "2. save the no labels in a seperate folder\n",
    "3. explode/expand the iabcategories for each url -> one line per iabcategory per url\n",
    "4. add the scores (high, medium, low) for each\n",
    "5. break url into base, domain, path\n",
    "6. combine the channel, domain, path together in one \n",
    "7. remove stop words (open source and custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2c4e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = ['www', 'www.', 'com', 'les', 'org', 'tag', 'html', 'id', 'un', 'win', \n",
    "                    'en', 'me', 'php', 'asp', 'aspx', 'cc', 'net']\n",
    "\n",
    "def get_iab_categories(iab_categories):\n",
    "    iab_categories_list = []\n",
    "    iab_categories = ast.literal_eval(iab_categories)\n",
    "    for iab_category in iab_categories:\n",
    "        for value in list(iab_category.values())[:-1]:\n",
    "            iab_categories_list.append([value, iab_category['score']])\n",
    "    return iab_categories_list\n",
    "\n",
    "def get_base_url_(url):\n",
    "    try:\n",
    "        return get_base_url(url)\n",
    "    except:\n",
    "        return url\n",
    "\n",
    "def get_domain(url):\n",
    "    try:\n",
    "        domain = parse_url(url)['domain']\n",
    "    except:\n",
    "        domain = ' '.join(urlparse(url).netloc.split('.'))\n",
    "    try:\n",
    "        top_domain = parse_url(url)['top_domain'] or ''\n",
    "    except:\n",
    "        top_domain = ''\n",
    "    try:\n",
    "        sub_domain = parse_url(url)['sub_domain'] or ''\n",
    "    except:\n",
    "        sub_domain = ''\n",
    "    return domain + ' ' + sub_domain + ' ' + top_domain \n",
    "    \n",
    "def get_path(url):\n",
    "    try:\n",
    "        path = parse_url(url)['path']\n",
    "    except:\n",
    "        path = urlparse(url).path\n",
    "    return path\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    # add lematize\n",
    "    stop = []\n",
    "    stop.extend(stopwords.words('english'))\n",
    "    stop.extend(stopwords.words('french'))\n",
    "    stop.extend(stopwords.words('spanish'))\n",
    "    stop.extend(custom_stopwords)\n",
    "    text = re.sub('[^A-Za-z]+', ' ', str(text)).strip()\n",
    "    text = ' '.join([word for word in text.split() if word not in (stop)])\n",
    "    \n",
    "    return text\n",
    "    \n",
    "def clean_df(df, df_name):  \n",
    "    lemmatizer = WordNetLemmatizer()  \n",
    "    df_nolabel = df[df['iabCategories'].isnull()]\n",
    "    df_nolabel.to_csv(f'../data/clean/no_label/{df_name}.csv')\n",
    "    \n",
    "    df = df[~df['iabCategories'].isnull()]\n",
    "    df = df.fillna('')\n",
    "    df['iabCategories'] = df['iabCategories'].apply(get_iab_categories)\n",
    "    df = df.explode('iabCategories', ignore_index = True)\n",
    "    split = pd.DataFrame(df['iabCategories'].to_list(), columns = ['iab_categories', 'confidence'])\n",
    "    df = pd.concat([df, split], axis = 1)\n",
    "    df = df.drop('iabCategories', axis = 1)\n",
    "    df['base_url'] = df['url'].apply(get_base_url_)\n",
    "    df['domain'] = df['url'].apply(get_domain) \n",
    "    df['path'] = df['url'].apply(get_path)\n",
    "    df['combine'] = df['channel'] + ' ' + df['domain'] + ' ' + df['path']\n",
    "    df['combine'] = df['combine'].apply(remove_stop_words)\n",
    "    df['combine'] = df['combine'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    df.to_csv(f'../data/clean/label/{df_name}.csv')\n",
    "    \n",
    "def clean():\n",
    "    for i in tqdm(range(1, 10)):\n",
    "        df = pd.read_csv(f'../data/csv/{i}.csv')\n",
    "        clean_df(df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d9d88c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 9/9 [4:04:58<00:00, 1633.20s/it]\n"
     ]
    }
   ],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa06afe5",
   "metadata": {},
   "source": [
    "### 4. Get data ready for training\n",
    "\n",
    "1. look at all the columns\n",
    "2. get rid of unnessary cols\n",
    "3. look at all the unique values in certain columns\n",
    "4. replace them with numerical classes\n",
    "5. join all csv files into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef517eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols_name():\n",
    "    cols = set()\n",
    "    for i in tqdm(range(1, 10)):\n",
    "        df = pd.read_csv(f'../data/clean/label/{i}.csv', low_memory = False)\n",
    "        cols.update(df.columns.values.tolist())\n",
    "    return cols\n",
    "\n",
    "def drop_cols():\n",
    "    cols_to_drop = ['Unnamed: 0', 'base_url', 'channel', 'domain', 'path', 'refDomain', 'url']\n",
    "    for i in tqdm(range(1, 10)):\n",
    "        df = pd.read_csv(f'../data/clean/label/{i}.csv', low_memory = False)\n",
    "        df = df.rename(columns = {'Unnamed: 0.1': 'id'})\n",
    "        df = df.set_index('id')\n",
    "        df = df.drop(cols_to_drop, axis = 1)\n",
    "        df.to_csv(f'../data/train/{i}.csv')\n",
    "        \n",
    "def get_unique_vals():\n",
    "    unique_vals = {\n",
    "        'browserFamily': set(),\n",
    "        'deviceType': set(),\n",
    "        'os': set(),\n",
    "        'confidence': set()\n",
    "    }\n",
    "    for i in tqdm(range(1, 10)):\n",
    "        df = pd.read_csv(f'../data/train/{i}.csv', low_memory = False)\n",
    "        for col in unique_vals.keys():\n",
    "            unique_vals[col].update(df[col].unique())\n",
    "    return unique_vals\n",
    "        \n",
    "    \n",
    "def display_df():\n",
    "    for i in range(1, 10):\n",
    "        df = pd.read_csv(f'../data/train/{i}.csv', low_memory = False)\n",
    "        display(df.head())\n",
    "        print()\n",
    "        \n",
    "def combine():\n",
    "    dfs = []\n",
    "    for i in tqdm(range(1, 10)):\n",
    "        df = pd.read_csv(f'../data/train/{i}.csv', low_memory = False)\n",
    "        dfs.append(df)\n",
    "    combined_df = pd.concat(dfs, axis=0, ignore_index = True)\n",
    "    combined_df.to_csv('../data/train/train_combined.csv', index = False)\n",
    "\n",
    "def group_val():\n",
    "    with open('../static_data/browser_family.pkl', 'rb') as f:\n",
    "        browser_family = pickle.load(f)\n",
    "    with open('../static_data/os.pkl', 'rb') as f:\n",
    "        os = pickle.load(f)\n",
    "    df = pd.read_csv('../data/train/train_combined.csv', low_memory = False)\n",
    "    df['browserFamily'] = df['browserFamily'].apply(lambda x: browser_family[x])\n",
    "    df['os'] = df['os'].apply(lambda x: os[x])\n",
    "    df.to_csv('../data/train/train_grouped.csv', index = False)\n",
    "    \n",
    "def replace_val_le():\n",
    "    df = pd.read_csv('../data/train/train_grouped.csv', low_memory = False)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(df['browserFamily'])\n",
    "    le_browser_family_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    with open('../data/static_data/browser_family_mapping.pkl', 'wb') as f:\n",
    "        pickle.dump(le_browser_family_mapping, f)\n",
    "    df['browserFamily'] = le.transform(df['browserFamily'])\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(df['deviceType'])\n",
    "    le_device_type_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    with open('../data/static_data/device_type_mapping.pkl', 'wb') as f:\n",
    "        pickle.dump(le_device_type_mapping, f)\n",
    "    df['deviceType'] = le.transform(df['deviceType'])\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(df['os'])\n",
    "    le_os_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    with open('../data/static_data/os_mapping.pkl', 'wb') as f:\n",
    "        pickle.dump(le_os_mapping, f)\n",
    "    df['os'] = le.transform(df['os'])\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(df['iab_categories'])\n",
    "    le_iab_categories_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    with open('../data/static_data/iab_categories_mapping.pkl', 'wb') as f:\n",
    "        pickle.dump(le_iab_categories_mapping, f)\n",
    "    df['iab_categories'] = le.transform(df['iab_categories'])\n",
    "    \n",
    "    df['confidence'] = df['confidence'].replace({'medium': 0.5, 'high': 1.0,})\n",
    "\n",
    "    df = df.dropna()\n",
    "    \n",
    "    df.to_csv('../data/train/train.csv', index = False)\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    try:\n",
    "        text = str(text)\n",
    "        text = ' '.join([lemmatizer.lemmatize(list(str(word))) for word in text.split()])\n",
    "        return text\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "def replace_val():\n",
    "    df = pd.read_csv('../data/train/train_grouped.csv', low_memory = False)\n",
    "    \n",
    "    le_browser_family_mapping = pickle.load(open('../static_data/browser_family_mapping.pkl', 'rb'))\n",
    "    le_device_type_mapping = pickle.load(open('../static_data/device_type_mapping.pkl', 'rb'))\n",
    "    le_os_mapping = pickle.load(open('../static_data/os_mapping.pkl', 'rb'))\n",
    "    le_iab_categories_mapping = pickle.load(open('../static_data/iab_categories_mapping.pkl', 'rb'))\n",
    "\n",
    "    df['browserFamily'] = df['browserFamily'].replace(le_browser_family_mapping)\n",
    "    df['deviceType'] = df['deviceType'].replace(le_device_type_mapping)\n",
    "    df['os'] = df['os'].replace(le_os_mapping)\n",
    "    df['iab_categories'] = df['iab_categories'].replace(le_iab_categories_mapping)\n",
    "    df['confidence'] = df['confidence'].replace({'medium': 0.5, 'high': 1.0,})\n",
    "    df['combine'] = df['combine'].apply(lemmatize)\n",
    "\n",
    "    df = df.dropna()\n",
    "    \n",
    "    df.to_csv('../data/train/train.csv', index = False)\n",
    "\n",
    "def get_train(show_cols = False, show_unique_vals = False):\n",
    "    # if show_cols:\n",
    "    #     pprint(get_cols_name())\n",
    "    # drop_cols()\n",
    "    # if show_unique_vals:\n",
    "    #     pprint(get_unique_vals())\n",
    "    # combine()\n",
    "    # group_val()\n",
    "    replace_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "042bc336",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50a2e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_alternate():\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cols_to_drop = ['browserFamily', 'deviceType', 'os', 'combine']\n",
    "    le_iab_categories_mapping = pickle.load(open('../static_data/iab_categories_mapping.pkl', 'rb'))\n",
    "\n",
    "    df = pd.read_csv('../data/train/train_combined.csv', low_memory = False)\n",
    "    df = df.fillna('')\n",
    "    df['X'] = df['browserFamily'].str.lower() + ' ' + df['deviceType'].str.lower() + ' ' + df['os'].str.lower() + ' ' + df['combine'].str.lower()\n",
    "    df['X'] = df['X'].apply(lemmatize)\n",
    "\n",
    "    df = df.drop(cols_to_drop, axis = 1)\n",
    "    \n",
    "    df['confidence'] = df['confidence'].replace({'medium': 0.5, 'high': 1.0,})\n",
    "    df['iab_categories'] = df['iab_categories'].replace(le_iab_categories_mapping)\n",
    "    df.to_csv('../data/train/train_alternate.csv', index = False)\n",
    "\n",
    "get_train_alternate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f54d7c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9143493, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>browserFamily</th>\n",
       "      <th>deviceType</th>\n",
       "      <th>os</th>\n",
       "      <th>iab_categories</th>\n",
       "      <th>confidence</th>\n",
       "      <th>combine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8133959</th>\n",
       "      <td>882044</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>flixhd genre romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3138929</th>\n",
       "      <td>33187</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>0.5</td>\n",
       "      <td>churchmouseyarns collections free patterns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579845</th>\n",
       "      <td>543285</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tn onedhs gov csp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599636</th>\n",
       "      <td>311265</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>166</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390188</th>\n",
       "      <td>390188</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>95</td>\n",
       "      <td>0.5</td>\n",
       "      <td>sukafilmxxi online silent night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851417</th>\n",
       "      <td>814857</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>157</td>\n",
       "      <td>0.5</td>\n",
       "      <td>face faceafrica article drug lord made k order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8021581</th>\n",
       "      <td>769666</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>1.0</td>\n",
       "      <td>prambors site movie pirates caribbean worlds end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6922507</th>\n",
       "      <td>709272</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>incestflix FD page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970309</th>\n",
       "      <td>899845</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>americanthinker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7835111</th>\n",
       "      <td>583196</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inverse entertainment time travel shows januar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  browserFamily  deviceType  os  iab_categories  confidence   \n",
       "8133959  882044              3         5.0   3              13         1.0  \\\n",
       "3138929   33187              3         6.0   3             140         0.5   \n",
       "1579845  543285              3         5.0   3             150         1.0   \n",
       "8599636  311265              0         3.0   5             166         1.0   \n",
       "390188   390188              0         3.0   7              95         0.5   \n",
       "1851417  814857              3         5.0   3             157         0.5   \n",
       "8021581  769666              3         5.0   0             151         1.0   \n",
       "6922507  709272              3         5.0   0             142         1.0   \n",
       "2970309  899845              0         3.0   5              95         1.0   \n",
       "7835111  583196              3         5.0   3              60         1.0   \n",
       "\n",
       "                                                   combine  \n",
       "8133959                               flixhd genre romance  \n",
       "3138929         churchmouseyarns collections free patterns  \n",
       "1579845                                  tn onedhs gov csp  \n",
       "8599636                                                NaN  \n",
       "390188                     sukafilmxxi online silent night  \n",
       "1851417  face faceafrica article drug lord made k order...  \n",
       "8021581   prambors site movie pirates caribbean worlds end  \n",
       "6922507                                 incestflix FD page  \n",
       "2970309                                    americanthinker  \n",
       "7835111  inverse entertainment time travel shows januar...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/train/train.csv')\n",
    "pprint(df.shape)\n",
    "df.sample(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62d753fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9143493, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>iab_categories</th>\n",
       "      <th>confidence</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5920754</th>\n",
       "      <td>741712</td>\n",
       "      <td>147</td>\n",
       "      <td>0.5</td>\n",
       "      <td>mobile safari smartphone ios elitedaily entert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8324386</th>\n",
       "      <td>36015</td>\n",
       "      <td>150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>chrome mobile smartphone ios bncollege mga login</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8857808</th>\n",
       "      <td>569437</td>\n",
       "      <td>142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mobile safari smartphone ios face faceafrica a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5172396</th>\n",
       "      <td>1030997</td>\n",
       "      <td>96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mobile safari smartphone ios face faceafrica a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8980773</th>\n",
       "      <td>692402</td>\n",
       "      <td>64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>chrome mobile smartphone android hotelcaesiust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3486096</th>\n",
       "      <td>380354</td>\n",
       "      <td>95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mobile safari smartphone ios unitedwater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2269816</th>\n",
       "      <td>199352</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>safari personal computer os x localconditions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6691436</th>\n",
       "      <td>478201</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mobile safari tablet ios google search naijapr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7089890</th>\n",
       "      <td>876655</td>\n",
       "      <td>65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mobile safari smartphone ios usatoday bengalsw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713383</th>\n",
       "      <td>642919</td>\n",
       "      <td>155</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mobile safari smartphone ios anime watch emine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  iab_categories  confidence   \n",
       "5920754   741712             147         0.5  \\\n",
       "8324386    36015             150         1.0   \n",
       "8857808   569437             142         1.0   \n",
       "5172396  1030997              96         1.0   \n",
       "8980773   692402              64         1.0   \n",
       "3486096   380354              95         1.0   \n",
       "2269816   199352              17         1.0   \n",
       "6691436   478201              13         1.0   \n",
       "7089890   876655              65         1.0   \n",
       "2713383   642919             155         1.0   \n",
       "\n",
       "                                                         X  \n",
       "5920754  mobile safari smartphone ios elitedaily entert...  \n",
       "8324386   chrome mobile smartphone ios bncollege mga login  \n",
       "8857808  mobile safari smartphone ios face faceafrica a...  \n",
       "5172396  mobile safari smartphone ios face faceafrica a...  \n",
       "8980773  chrome mobile smartphone android hotelcaesiust...  \n",
       "3486096           mobile safari smartphone ios unitedwater  \n",
       "2269816  safari personal computer os x localconditions ...  \n",
       "6691436  mobile safari tablet ios google search naijapr...  \n",
       "7089890  mobile safari smartphone ios usatoday bengalsw...  \n",
       "2713383  mobile safari smartphone ios anime watch emine...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/train/train_alternate.csv')\n",
    "pprint(df.shape)\n",
    "df.sample(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f79d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
